{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dự án thực hành\n",
        "```\n",
        "ProtonX - TF Project\n",
        "```\n",
        "\n",
        "### Hướng dẫn làm bài\n",
        "- Trong bài tập này bạn sẽ sử dụng Python 3.\n",
        "- Sau khi bạn viết Code của mình xong, hãy chạy dòng Code đó để xem kết quả bên dưới.\n",
        "\n",
        "### [Quan trọng] Chú ý\n",
        "- **Không sử dụng hàm `input()` tại bất kỳ dòng lệnh nào**\n",
        "- **Không thay đổi dòng code return của hàm**\n",
        "\n",
        "Các bạn sẽ thực hiện `code` trong các phần hiển thị `#TODO: Lập trình tại đây` và thay thế các vị trí `None`. Có những câu hỏi chỉ cần trả về đáp án.\n",
        "\n",
        "Sau khi viết xong Code của bạn, bạn hãy ấn \"SHIFT\"+\"ENTER\" để thực hiện chạy lệnh của Cell đó.\n",
        "\n",
        "---\n",
        "Điểm số:\n",
        "* 10 điểm / Câu\n",
        "\n",
        "Tiêu chí chấm điểm:\n",
        "* Các bài tập sẽ được chấm dựa trên các Test-case.\n",
        "* Các bạn không khởi tạo lại giá trị đầu vào bên trong hàm. Có thể khởi tạo các giá trị này ngoài hàm nhằm mục đích kiểm thử."
      ],
      "metadata": {
        "id": "oQPiD2EfqvJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mục tiêu của bài tập này:** Bài tập này hướng dẫn bạn cách sử dụng Transformer Encoder để thực hiện bài toán phân loại\n",
        "\n",
        "Một số thông tin:\n",
        "- Dữ liệu: IMDB Reviews với đầu vào là câu và nhãn là tích cực hay tiêu cực\n",
        "- Mô hình phân loại nhị phân\n",
        "\n",
        "Để làm được bài tập này, bạn cần xem kỹ [video sau](https://youtu.be/_Zt23FA31co)."
      ],
      "metadata": {
        "id": "GlY6Tc3drZfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D"
      ],
      "metadata": {
        "id": "aJxa3EUWgkT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tham số\n",
        "vocab_size = 10000\n",
        "maxlen = 200\n",
        "embedding_dim = 32\n",
        "num_heads = 2\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_encoder_layers = 2\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "\n",
        "# Load and preprocess the IMDB dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Customized Transformer model for IMDB reviews classification"
      ],
      "metadata": {
        "id": "0Pc3-OafrY9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chiều của bộ dữ liệu"
      ],
      "metadata": {
        "id": "sFQmJZzWrpcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNzFEmqlrsCH",
        "outputId": "2427da20-3a0c-47bf-e73c-a6ab2191e687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000, 200), (25000,))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 1: Lập trình công thức Attention"
      ],
      "metadata": {
        "id": "k90WURinl5bY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://storage.googleapis.com/mle-courses-prod/users/61b6fa1ba83a7e37c8309756/private-files/9dc89b20-e4fe-11ee-8e78-a7a9c4d473e7-Screen_Shot_2024_03_18_at_15.07.47.png)"
      ],
      "metadata": {
        "id": "TIYa0Fevmmxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    # Lập trình tại đây\n",
        "    output = None\n",
        "    return output"
      ],
      "metadata": {
        "id": "d88f52-9gim0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test code"
      ],
      "metadata": {
        "id": "1e7lv_ejhA5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_output_shape():\n",
        "    num_queries = 10\n",
        "    num_keys = 15\n",
        "    dim = 64\n",
        "\n",
        "    q = tf.random.normal(shape=(num_queries, dim))\n",
        "    k = tf.random.normal(shape=(num_keys, dim))\n",
        "    v = tf.random.normal(shape=(num_keys, dim))\n",
        "    mask = None\n",
        "\n",
        "    output = scaled_dot_product_attention(q, k, v, mask)\n",
        "    assert output.shape == (num_queries, dim), \"Output shape test failed\"\n",
        "    print(\"Test output shape passed.\")\n",
        "\n",
        "def test_mask_effectiveness():\n",
        "    num_queries = 2\n",
        "    num_keys = 2\n",
        "    dim = 64\n",
        "\n",
        "    q = tf.random.normal(shape=(num_queries, dim))\n",
        "    k = tf.random.normal(shape=(num_keys, dim))\n",
        "    v = tf.random.normal(shape=(num_keys, dim))\n",
        "\n",
        "    no_mask_output = scaled_dot_product_attention(q, k, v, None)\n",
        "\n",
        "    mask = tf.constant([[[0.0], [-1e9]]])\n",
        "    masked_output = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "    difference = tf.reduce_sum(tf.abs(no_mask_output - masked_output))\n",
        "    assert difference.numpy() != 0, \"Mask effectiveness test failed\"\n",
        "    print(\"Test mask effectiveness passed.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-qtbKbXjhAqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_output_shape()\n",
        "test_mask_effectiveness()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a9ZQak9hApW",
        "outputId": "5ed93019-7ed4-47f6-940d-c843a35822ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test output shape passed.\n",
            "Test mask effectiveness passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 2: Lập trình multi head Attention"
      ],
      "metadata": {
        "id": "fAVGNnWNmsIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://storage.googleapis.com/mle-courses-prod/users/61b6fa1ba83a7e37c8309756/private-files/08631a50-e4ff-11ee-b19e-958156644f33-Screen_Shot_2024_03_18_at_15.10.46.png)"
      ],
      "metadata": {
        "id": "7x0U0zc1nNPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = None\n",
        "        self.d_model = None\n",
        "        assert d_model % self.num_heads == 0\n",
        "        self.depth = None\n",
        "        self.wq = None\n",
        "        self.wk = None\n",
        "        self.wv = None\n",
        "        self.dense = None\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        # Lâp trình tại đây\n",
        "        return x\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        # Lâp trình tại đây\n",
        "        output = None\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "C1yOrZOmgljk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "class TestMultiHeadAttention(unittest.TestCase):\n",
        "    def test_initialization(self):\n",
        "        \"\"\"Test layer initialization.\"\"\"\n",
        "        d_model = 128\n",
        "        num_heads = 8\n",
        "        mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.assertEqual(mha.d_model, d_model)\n",
        "        self.assertEqual(mha.num_heads, num_heads)\n",
        "        print('Test passed')\n",
        "\n",
        "    def test_output_shape(self):\n",
        "        \"\"\"Test the shape of the output tensor.\"\"\"\n",
        "        batch_size = 1\n",
        "        seq_length = 60\n",
        "        d_model = 128\n",
        "        num_heads = 8\n",
        "        mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        v = k = q = tf.random.uniform((batch_size, seq_length, d_model), dtype=tf.float32)\n",
        "        mask = None\n",
        "        output = mha(v, k, q, mask)\n",
        "        self.assertEqual(output.shape, (batch_size, seq_length, d_model))\n",
        "        print('Test passed')\n",
        "\n",
        "    def test_d_model_num_heads_compatibility(self):\n",
        "        \"\"\"Ensure layer checks for compatibility between d_model and num_heads.\"\"\"\n",
        "        with self.assertRaises(AssertionError):\n",
        "            MultiHeadAttention(d_model=128, num_heads=6)\n",
        "        print('Test passed')\n",
        "\n",
        "\n",
        "    def test_split_heads_shape(self):\n",
        "        \"\"\"Test the shape after splitting heads.\"\"\"\n",
        "        batch_size = 1\n",
        "        seq_length = 60\n",
        "        d_model = 128\n",
        "        num_heads = 8\n",
        "        mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        x = tf.random.uniform((batch_size, seq_length, d_model), dtype=tf.float32)\n",
        "        split_x = mha.split_heads(x, batch_size)\n",
        "        expected_shape = (batch_size, num_heads, seq_length, d_model // num_heads)\n",
        "        self.assertEqual(split_x.shape, expected_shape)\n",
        "        print('Test passed')\n",
        "\n",
        "\n",
        "    def test_attention_calculation(self):\n",
        "        \"\"\"Test attention mechanism with dummy mask.\"\"\"\n",
        "        batch_size = 1\n",
        "        seq_length = 60\n",
        "        d_model = 128\n",
        "        num_heads = 8\n",
        "        mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        v = k = q = tf.random.uniform((batch_size, seq_length, d_model), dtype=tf.float32)\n",
        "        mask = tf.zeros((batch_size, 1, 1, seq_length))\n",
        "        output = mha(v, k, q, mask)\n",
        "        self.assertNotEqual(tf.reduce_sum(output), 0)\n",
        "        print('Test passed')\n",
        "\n",
        "\n",
        "    def test_integration_with_tf_model(self):\n",
        "        \"\"\"Test the integration of MultiHeadAttention within a TensorFlow model.\"\"\"\n",
        "        class TestModel(tf.keras.Model):\n",
        "            def __init__(self, d_model, num_heads):\n",
        "                super(TestModel, self).__init__()\n",
        "                self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "            def call(self, inputs, mask=None):\n",
        "                return self.mha(inputs, inputs, inputs, mask)\n",
        "\n",
        "        d_model = 128\n",
        "        num_heads = 8\n",
        "        test_model = TestModel(d_model=d_model, num_heads=num_heads)\n",
        "        v = k = q = tf.random.uniform((1, 60, d_model), dtype=tf.float32)\n",
        "        mask = None\n",
        "        output = test_model(v, mask=mask)\n",
        "        self.assertEqual(output.shape, (1, 60, d_model))\n",
        "        print('Test passed')\n",
        "\n",
        "\n",
        "testMultiHeadAttention = TestMultiHeadAttention()\n",
        "testMultiHeadAttention.test_initialization()\n",
        "testMultiHeadAttention.test_output_shape()\n",
        "testMultiHeadAttention.test_d_model_num_heads_compatibility()\n",
        "testMultiHeadAttention.test_split_heads_shape()\n",
        "testMultiHeadAttention.test_attention_calculation()\n",
        "testMultiHeadAttention.test_integration_with_tf_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kgDjTZ9iQab",
        "outputId": "854155bf-8987-4bfc-ae59-cdbde539f4b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed\n",
            "Test passed\n",
            "Test passed\n",
            "Test passed\n",
            "Test passed\n",
            "Test passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "metadata": {
        "id": "eWTtkbOhgnaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 3: Lập trình lớp Encoder\n",
        "\n",
        "![](https://storage.googleapis.com/mle-courses-prod/users/61b6fa1ba83a7e37c8309756/private-files/279406f0-e4ff-11ee-8e78-a7a9c4d473e7-Screen_Shot_2024_03_18_at_15.11.38.png)"
      ],
      "metadata": {
        "id": "BFxS2Eb4nfic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # Lập trình tại đây\n",
        "        self.mha = None\n",
        "        self.ffn = None\n",
        "        self.layernorm1 = None\n",
        "        self.layernorm2 = None\n",
        "        self.dropout1 = None\n",
        "        self.dropout2 = None\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        # Lập trình tại đây\n",
        "        out2 = None\n",
        "        return out2\n"
      ],
      "metadata": {
        "id": "dvDJasTfgo1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestEncoderLayer(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.d_model = 128\n",
        "        self.num_heads = 8\n",
        "        self.dff = 512\n",
        "        self.rate = 0.1\n",
        "        self.encoder_layer = EncoderLayer(d_model=self.d_model, num_heads=self.num_heads, dff=self.dff, rate=self.rate)\n",
        "\n",
        "    def test_initialization(self):\n",
        "        \"\"\"Test if the EncoderLayer initializes correctly.\"\"\"\n",
        "        self.assertIsInstance(self.encoder_layer.mha, MultiHeadAttention)\n",
        "        self.assertIsInstance(self.encoder_layer.ffn, tf.keras.Sequential)  # Assuming ffn returns a Sequential model\n",
        "        # Additional checks for layer sizes, rates, etc., can be added here.\n",
        "\n",
        "    def test_output_shape(self):\n",
        "        \"\"\"Ensure the output tensor has the correct shape.\"\"\"\n",
        "        sample_input = tf.random.uniform((1, 60, self.d_model))\n",
        "        output = self.encoder_layer(sample_input, training=False, mask=None)\n",
        "        self.assertEqual(output.shape, (1, 60, self.d_model))\n",
        "\n",
        "    def test_layer_normalization_equality(self):\n",
        "        \"\"\"Layer normalization should not alter zero tensors.\"\"\"\n",
        "        zero_input = tf.zeros((1, 60, self.d_model))\n",
        "        output = self.encoder_layer(zero_input, training=False, mask=None)\n",
        "        self.assertTrue(tf.reduce_all(output == 0))\n",
        "\n",
        "    def test_dropout_effect_in_training_mode(self):\n",
        "        \"\"\"Dropout should create variance in output during training.\"\"\"\n",
        "        sample_input = tf.random.uniform((1, 60, self.d_model), seed=42)\n",
        "        output1 = self.encoder_layer(sample_input, training=True, mask=None)\n",
        "        output2 = self.encoder_layer(sample_input, training=True, mask=None)\n",
        "        difference = tf.reduce_mean(tf.abs(output1 - output2))\n",
        "        self.assertNotEqual(difference.numpy(), 0)\n",
        "\n",
        "    def test_effectiveness_of_masking(self):\n",
        "        \"\"\"Applying a mask should influence the output.\"\"\"\n",
        "        sample_input = tf.random.uniform((1, 60, self.d_model))\n",
        "        mask = tf.cast(tf.math.equal(tf.random.uniform((1, 1, 60), maxval=2, dtype=tf.int32), 1), tf.float32)\n",
        "        output_no_mask = self.encoder_layer(sample_input, training=False, mask=None)\n",
        "        output_with_mask = self.encoder_layer(sample_input, training=False, mask=mask)\n",
        "        self.assertNotEqual(tf.reduce_sum(output_no_mask).numpy(), tf.reduce_sum(output_with_mask).numpy())\n",
        "\n",
        "testEncoderLayer = TestEncoderLayer()\n",
        "testEncoderLayer.setUp()\n",
        "testEncoderLayer.test_initialization()\n",
        "testEncoderLayer.test_output_shape()\n",
        "testEncoderLayer.test_layer_normalization_equality()\n",
        "testEncoderLayer.test_dropout_effect_in_training_mode()\n",
        "testEncoderLayer.test_effectiveness_of_masking()"
      ],
      "metadata": {
        "id": "EnaJC_XWjhrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 4: Lập trình Encoder cho vị trí\n",
        "\n",
        "![](https://storage.googleapis.com/mle-courses-prod/users/61b6fa1ba83a7e37c8309756/private-files/5f04af40-e4ff-11ee-9cb8-b304dc1ec8d0-Screen_Shot_2024_03_18_at_15.13.10.png)"
      ],
      "metadata": {
        "id": "Xcyf4Xn2nl1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    def get_angles(pos, i, d_model):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "        return pos * angle_rates\n",
        "\n",
        "    # Lập trình tại đây\n",
        "    angle_rads = None\n",
        "\n",
        "    # Apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = None\n",
        "\n",
        "    # Apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = None\n",
        "\n",
        "    pos_encoding = None\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ],
      "metadata": {
        "id": "el3_faQ5dvXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestPositionalEncoding(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.position = 50\n",
        "        self.d_model = 512\n",
        "\n",
        "    def test_output_shape(self):\n",
        "        \"\"\"Test that the output shape is correct.\"\"\"\n",
        "        pos_encoding = positional_encoding(self.position, self.d_model)\n",
        "        expected_shape = (1, self.position, self.d_model)\n",
        "        self.assertEqual(pos_encoding.shape, expected_shape)\n",
        "\n",
        "    def test_output_type(self):\n",
        "        \"\"\"Test that the output type is tf.float32.\"\"\"\n",
        "        pos_encoding = positional_encoding(self.position, self.d_model)\n",
        "        self.assertTrue(isinstance(pos_encoding, tf.Tensor))\n",
        "        self.assertEqual(pos_encoding.dtype, tf.float32)\n",
        "\n",
        "    def test_even_indices_using_sin(self):\n",
        "        \"\"\"Test even indices use sin.\"\"\"\n",
        "        pos_encoding = positional_encoding(1, 4)  # Smaller size for easy test\n",
        "        # Extract first position's even indices encoding\n",
        "        even_encoding = pos_encoding[0, 0, 0::2].numpy()\n",
        "        self.assertTrue(np.allclose(even_encoding, np.sin([0, 0]), atol=1e-6))\n",
        "\n",
        "    def test_odd_indices_using_cos(self):\n",
        "        \"\"\"Test odd indices use cos.\"\"\"\n",
        "        pos_encoding = positional_encoding(1, 4)  # Smaller size for easy test\n",
        "        # Extract first position's odd indices encoding\n",
        "        odd_encoding = pos_encoding[0, 0, 1::2].numpy()\n",
        "        self.assertTrue(np.allclose(odd_encoding, np.cos([0, 0]), atol=1e-6))\n",
        "\n",
        "    def test_first_position(self):\n",
        "        \"\"\"Test the encoding for the first position.\"\"\"\n",
        "        pos_encoding = positional_encoding(1, 2)  # Single position, two dimensions\n",
        "        expected_encoding = np.array([[np.sin(0), np.cos(0)]])\n",
        "        self.assertTrue(np.allclose(pos_encoding[0, 0].numpy(), expected_encoding, atol=1e-6))\n",
        "\n",
        "    def test_uniqueness(self):\n",
        "        \"\"\"Test that encodings for different positions are unique.\"\"\"\n",
        "        pos_encoding = positional_encoding(self.position, self.d_model)\n",
        "        # Flatten the positional encoding to compare each position's encoding vector\n",
        "        pos_encoding_flat = tf.reshape(pos_encoding, (-1, self.d_model)).numpy()\n",
        "        # Check if all rows are unique\n",
        "        self.assertEqual(len(np.unique(pos_encoding_flat, axis=0)), self.position)\n",
        "\n",
        "testPositionalEncoding = TestPositionalEncoding()\n",
        "testPositionalEncoding.setUp()\n",
        "testPositionalEncoding.test_output_shape()\n",
        "testPositionalEncoding.test_output_type()\n",
        "testPositionalEncoding.test_even_indices_using_sin()\n",
        "testPositionalEncoding.test_odd_indices_using_cos()\n",
        "testPositionalEncoding.test_first_position()\n",
        "testPositionalEncoding.test_uniqueness()"
      ],
      "metadata": {
        "id": "g2BL1UQ7kscI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 5: Đóng gói Encoder"
      ],
      "metadata": {
        "id": "ARe6UxBksQwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Với bài toán này, sau khi đã học được embedding của các câu. Ta tổng hợp các embedding thành một embedding chung và đưa vào trong một mô hình phân loại."
      ],
      "metadata": {
        "id": "7oah4ChIq_Eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://storage.googleapis.com/mle-courses-prod/users/61b6fa1ba83a7e37c8309756/private-files/ee52a0d0-e4ff-11ee-8e78-a7a9c4d473e7-Screen_Shot_2024_03_18_at_15.17.12.png)"
      ],
      "metadata": {
        "id": "mV-fNQCCoxZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerClassifier(tf.keras.Model):\n",
        "    def __init__(self, num_encoder_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        # Lập trình tại đây\n",
        "        self.embedding = None\n",
        "        self.pos_encoding = None\n",
        "        self.enc_layers = None\n",
        "        self.dropout = None\n",
        "        self.global_average_pooling = None\n",
        "        self.final_layer = None\n",
        "\n",
        "    def call(self, x, training):\n",
        "        # Lập trình tại đây\n",
        "        return None"
      ],
      "metadata": {
        "id": "u1ZuKqESlNBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestTransformerClassifier(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.num_encoder_layers = 2\n",
        "        self.d_model = 128\n",
        "        self.num_heads = 4\n",
        "        self.dff = 512\n",
        "        self.input_vocab_size = 10000\n",
        "        self.maximum_position_encoding = 1000\n",
        "        self.rate = 0.1\n",
        "        self.transformer_classifier = TransformerClassifier(self.num_encoder_layers, self.d_model, self.num_heads, self.dff, self.input_vocab_size, self.maximum_position_encoding, self.rate)\n",
        "\n",
        "    def test_output_shape(self):\n",
        "        \"\"\"Ensure the model outputs the correct shape.\"\"\"\n",
        "        batch_size = 32\n",
        "        sequence_length = 50\n",
        "        x = tf.random.uniform((batch_size, sequence_length), dtype=tf.int64, minval=0, maxval=self.input_vocab_size)\n",
        "        output = self.transformer_classifier(x, training=False)\n",
        "        self.assertEqual(output.shape, (batch_size, 1))\n",
        "\n",
        "    def test_output_type(self):\n",
        "        \"\"\"Test that the output type is tf.float32.\"\"\"\n",
        "        x = tf.random.uniform((1, 10), dtype=tf.int64, minval=0, maxval=self.input_vocab_size)\n",
        "        output = self.transformer_classifier(x, training=False)\n",
        "        self.assertTrue(isinstance(output, tf.Tensor))\n",
        "        self.assertEqual(output.dtype, tf.float32)\n",
        "\n",
        "    def test_training_flag_effect(self):\n",
        "        \"\"\"Dropout should only be active during training.\"\"\"\n",
        "        x = tf.random.uniform((1, 10), dtype=tf.int64, minval=0, maxval=self.input_vocab_size)\n",
        "        output_training = self.transformer_classifier(x, training=True)\n",
        "        output_inference = self.transformer_classifier(x, training=False)\n",
        "        self.assertNotEqual(tf.reduce_mean(output_training), tf.reduce_mean(output_inference), \"Dropout not applied correctly based on training flag.\")\n",
        "\n",
        "\n",
        "testTransformerClassifier = TestTransformerClassifier()\n",
        "testTransformerClassifier.setUp()\n",
        "testTransformerClassifier.test_output_shape()\n",
        "testTransformerClassifier.test_output_type()\n",
        "testTransformerClassifier.test_training_flag_effect()\n"
      ],
      "metadata": {
        "id": "9sRAUhlUlNiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiến hành đào tạo mô hình"
      ],
      "metadata": {
        "id": "dxXypcE4sU6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = TransformerClassifier(\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=vocab_size,\n",
        "    maximum_position_encoding=maxlen\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss=BinaryCrossentropy(), metrics=[BinaryAccuracy()])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGwNqtlwdBu4",
        "outputId": "a99472b9-4409-4acd-861e-b0485012ff71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 51s 54ms/step - loss: 0.5829 - binary_accuracy: 0.6355 - val_loss: 0.3128 - val_binary_accuracy: 0.8675\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.2738 - binary_accuracy: 0.8865 - val_loss: 0.2976 - val_binary_accuracy: 0.8736\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.2054 - binary_accuracy: 0.9203 - val_loss: 0.2904 - val_binary_accuracy: 0.8790\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 18s 23ms/step - loss: 0.1654 - binary_accuracy: 0.9383 - val_loss: 0.3600 - val_binary_accuracy: 0.8666\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 17s 22ms/step - loss: 0.1303 - binary_accuracy: 0.9538 - val_loss: 0.4011 - val_binary_accuracy: 0.8592\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 18s 23ms/step - loss: 0.1021 - binary_accuracy: 0.9639 - val_loss: 0.4463 - val_binary_accuracy: 0.8544\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 17s 22ms/step - loss: 0.0847 - binary_accuracy: 0.9705 - val_loss: 0.4453 - val_binary_accuracy: 0.8523\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 17s 22ms/step - loss: 0.0712 - binary_accuracy: 0.9745 - val_loss: 0.6788 - val_binary_accuracy: 0.8292\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 23s 29ms/step - loss: 0.0603 - binary_accuracy: 0.9784 - val_loss: 0.6067 - val_binary_accuracy: 0.8418\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 18s 23ms/step - loss: 0.0558 - binary_accuracy: 0.9813 - val_loss: 0.6922 - val_binary_accuracy: 0.8259\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.6922 - binary_accuracy: 0.8259\n",
            "Test Loss: 0.6921525597572327, Test Accuracy: 0.8259199857711792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kết quả trên bài toán phân loại: 87-88%"
      ],
      "metadata": {
        "id": "FxselIDNsXiT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p7fJK9FxsZfl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}